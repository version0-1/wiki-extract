# 実際の値は入れず、.env にコピーして設定。.env は .gitignore 済み。Docker は env_file でここを読む。

# --- デフォルトの LLM プロバイダ・モデル ---
# WIKI_LLM_PROVIDER=gemini
# WIKI_LLM_DEFAULT_MODEL_OLLAMA=gemma3:4b
# WIKI_LLM_DEFAULT_MODEL_GEMINI=gemini-2.5-flash-lite

# --- LLM バッチ・並列・タイムアウト ---
# WIKI_LLM_FILTER_BATCH_SIZE=30
# WIKI_LLM_SPLIT_BATCH_SIZE=30
# WIKI_LLM_WORKERS=1
# WIKI_LLM_TIMEOUT=300

# --- Ollama（WIKI_LLM_PROVIDER = ollama の場合）---
# LinuxでlocalhostのOllamaを使用する場合はhttp://localhost:11434を設定して以下コマンドで起動する
# docker compose -f docker-compose.yml -f docker-compose_linux.yml up -d
# LLM_OLLAMA_BASE_URL=http://host.docker.internal:11434

# --- Vertex AI（WIKI_LLM_PROVIDER = gemini の場合）---
## Vertex AI のベース URL
# LLM_GEMINI_BASE_URL=https://us-central1-aiplatform.googleapis.com
GEMINI_API_KEY=your_vertex_api_key_here
# GEMINI_RETRY_ATTEMPTS=3
# GEMINI_RETRY_BACKOFF=2.0

# --- パス（extract-pages / extract-character-candidates 等）---
# WIKI_DATA_DIR=/data
# WIKI_OUTPUT_DIR=/out
# WIKI_INPUT_DIR=/out
# WIKI_EXCLUDE_LIST=/path/to/excluded_names.json
